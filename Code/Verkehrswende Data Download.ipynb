{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b85999d3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Get Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3abf9db",
   "metadata": {},
   "source": [
    "I get data from Twitter in 2021 and 2022.\n",
    "\n",
    "These are queries for both the 9-Euro-Ticket and Verkehrswende more generally. I've downloaded the Verkehrswende data twice, the code below is an update from 2022 (the first file is from 2021)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d357dd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "!twarc2 searches --archive --start-time 2022-03-23 --end-time 2022-07-15 \"../Verkehrswende-Vergleich/Code/Queries/9euroticket.txt\" \"../Verkehrswende-Vergleich/Data/9euroticket.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d617cddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!twarc2 search --start-time 2021-06-16 --end-time 2022-7-15 --archive \"Verkehrswende OR Mobilitätswende -is:retweet\" > \"../Verkehrswende-Vergleich/Data/Verkehrswende_2022.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd9c107",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! twarc2 csv --extra-input-columns \"author.withheld.scope\" \"../Verkehrswende-Vergleich/Data/Verkehrswende_2022.jsonl\" \"../Verkehrswende-Vergleich/Data/Verkehrswende_2022.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b177147",
   "metadata": {},
   "outputs": [],
   "source": [
    "! twarc2 csv --extra-input-columns \"author.withheld.scope\" \"../Verkehrswende-Vergleich/Data/9euroticket.jsonl\" \"../Verkehrswende-Vergleich/Data/9euroticket.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913decc7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Datawrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8379cc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def statistics_tweet(df):\n",
    "    '''\n",
    "    This script transforms an existing standard twarc2 Twitter dataset to allow for convenient further analysis.\n",
    "    Two major results are produced. \n",
    "    One, ratios are calculated that ve meaning to tweets' success. Second, hashtags, mentions and urls are extracted from the JSON tagstrings.\n",
    "    Besides, follower, like and retweet columns are renamed and float numbers transformed into integers.\n",
    "    '''\n",
    "    # Get orinal columns\n",
    "    col = len(df.columns)\n",
    "    \n",
    "    # Turn off SettingWithCopyWarning because we indeed want to transform the orinal dataframe\n",
    "    pd.set_option('mode.chained_assignment', None)\n",
    "    \n",
    "    # Create a variable: follower/following ratio\n",
    "    df['foll_ratio'] = (df['author.public_metrics.followers_count'] / df['author.public_metrics.following_count']) #if df[(df['author.public_metrics.following_count'] != 0)] else 0\n",
    "        \n",
    "    # Create a variable: likes/follower\n",
    "    df['like_foll'] = (df['public_metrics.like_count'] / df['author.public_metrics.followers_count']) #if df[(df['author.public_metrics.followers_count'] != 0)] else 0\n",
    "\n",
    "    # Delete false data (which include only NaNs) \n",
    "    pd.set_option('mode.use_inf_as_na', True)\n",
    "    df.dropna(how='all') \n",
    "    df.dropna(subset=['public_metrics.retweet_count'], inplace=True)\n",
    "    df.dropna(subset=['public_metrics.like_count'], inplace=True)\n",
    "    df.dropna(subset=['author.public_metrics.followers_count'], inplace=True)\n",
    "    \n",
    "    # Create and make columns more human-readable\n",
    "    df['entities.mentions'] = df['entities.mentions'].astype(str)\n",
    "    df['entities.urls'] = df['entities.urls'].astype(str)\n",
    "    df['entities.hashtags'] = df['entities.hashtags'].astype(str)\n",
    "    df['retweets'] = df['public_metrics.retweet_count'].astype(int)\n",
    "    df['likes'] = df['public_metrics.like_count'].astype(int)\n",
    "    df['followers'] = df['author.public_metrics.followers_count'].astype(int)\n",
    "    df.drop(['public_metrics.retweet_count', 'public_metrics.like_count', 'author.public_metrics.followers_count', 'attachments.media_keys', 'attachments.poll.end_datetime', 'attachments.poll.id', 'attachments.poll.options', 'attachments.poll.voting_status'], axis=1, inplace=True)\n",
    "    \n",
    "    # collect list of mentions, hashtags and urls\n",
    "    \n",
    "    def find_mentions(tagstring):\n",
    "        false = False\n",
    "        true = True   \n",
    "        try:\n",
    "            if tagstring == tagstring:\n",
    "                list_of_dicts = eval(tagstring)\n",
    "                mentions = []\n",
    "                for dct in list_of_dicts:\n",
    "                    tag = dct['username']\n",
    "                    mentions.append(tag)\n",
    "                return mentions\n",
    "            else:\n",
    "                return np.nan\n",
    "        except:\n",
    "            tagstring is None\n",
    "    \n",
    "    df['mentions'] = df['entities.mentions'].apply(find_mentions, lambda col: col.str.lower())\n",
    "\n",
    "    def find_urls(tagstring):\n",
    "        try:\n",
    "            if tagstring == tagstring:\n",
    "                list_of_dicts = eval(tagstring)\n",
    "                urls = []\n",
    "                for dct in list_of_dicts:\n",
    "                    tag = dct['expanded_url']\n",
    "                    urls.append(tag)\n",
    "                return urls\n",
    "            else:\n",
    "                return np.nan\n",
    "        except:\n",
    "            tagstring is None\n",
    "            \n",
    "    df['urls'] = df['entities.urls'].apply(find_urls, lambda col: col.str.lower())\n",
    "\n",
    "    def find_hashtags(tagstring):\n",
    "        try:\n",
    "            if tagstring == tagstring:\n",
    "                list_of_dicts = eval(tagstring)\n",
    "                hashtags = []\n",
    "                for dct in list_of_dicts:\n",
    "                    tag = dct['tag']\n",
    "                    hashtags.append(tag)\n",
    "                return hashtags\n",
    "            else:\n",
    "                return np.nan\n",
    "        except:\n",
    "            tagstring is None\n",
    "\n",
    "    df['hashtags'] = df['entities.hashtags'].apply(find_hashtags, lambda col: df['hashtags'].str.lower())\n",
    "    \n",
    "    # prepare data for visualization\n",
    "    df['id'] = df['id'].astype(str)\n",
    "    df['hashtags'] = df['hashtags'].astype(str)\n",
    "    df['name'] = df['author.username'] + ' ' + df['hashtags']\n",
    "    df['type'] = df['type'].fillna('Tweet')\n",
    "    df['type'] = df['type'].replace(to_replace =[''], value ='Tweet')\n",
    "    \n",
    "    # prepara data for Gephi export (remove list-type), thus allowing the Gephi import with spaces\n",
    "    df['clean_urls'] = df['urls'].astype(str).str.replace(r'\\[|\\]|,', '', regex=True)\n",
    "    df['clean_urls'] = df['clean_urls'].astype(str).str.replace(r\"'\", \"\", regex=True)\n",
    "    df['clean_hashtags'] = df['hashtags'].astype(str).str.replace(r'\\[|\\]|,', '', regex=True)\n",
    "    df['clean_hashtags'] = df['clean_hashtags'].astype(str).str.replace(r\"'\", \"\", regex=True)\n",
    "    df['clean_mentions'] = df['mentions'].astype(str).str.replace(r'\\[|\\]|,', '', regex=True)\n",
    "    df['clean_mentions'] = df['clean_mentions'].astype(str).str.replace(r\"'\", \"\", regex=True)\n",
    "    \n",
    "    return print('Dataframe with basic statistics transformed. Hashtags, mentions and urls parsed.\\nOrinal count of columns: {}, new count of colums: {}.'.format(col,len(df.columns)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f5a8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics_tweet(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b546a3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_hashtags']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9928d9df",
   "metadata": {},
   "source": [
    "Dataframe gets written into a CSV-file to save and further analyse with pandas' '.to_csv'-function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8d58d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"../Verkehrswende-Vergleich/Data/Verkehrswende_2022_transformed.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d3367a-a6ab-4305-9998-cf7e6e410728",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Merge datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5521b46-dcdb-42cc-9953-304cc9deac44",
   "metadata": {},
   "source": [
    "Nun lt es, die bisherigen Daten mit den neuen Daten zusammenzubringen -- dafür müssen wie passbar gemacht werden. Erste Frage: Was ist der richtige Datensatz?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25253786-4146-4196-8396-5988cabb4e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../Verkehrswende-Vergleich/Data/Verkehrswende/Verkehrswende_transformed.csv\", low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fc053b-d50f-4622-8973-2bc833e9a793",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency = get_tweet_frequency(data_path, \"Verkehrswende\")\n",
    "frequency.loc[frequency['tweet_count'].idxmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58543a6f-852a-47c1-92b9-e360cc2d8ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = pd.read_csv(\"../Verkehrswende-Vergleich/Data/Verkehrswende_2022_transformed.csv\", low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea93aaaf-2482-4852-b576-e045e14dac6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns.difference(df_.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce05edad-4f5e-48cc-a8d0-cba6810bc76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_.columns.difference(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ef87a7-fbcd-4705-bbad-561483ea85b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['author.withheld.copyright', 'Unnamed: 0.1', 'in_reply_to_user.withheld.scope'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1c31c2-30f9-42ef-b11d-667512460d9b",
   "metadata": {},
   "source": [
    "Der Datensatz passt. Nun lt es, zu verbinden mit `pd.concat`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edaaff1-bb81-41d2-b02e-892f39608a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_['created_at'].head(-20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d44e5b8-095f-41c8-be74-d1f7b0f042f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the appropriate method for concatenating.\n",
    "df_concat = pd.concat([df, df_], join=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d8af82-5281-4ebb-b889-53162ade2f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_index(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b23495e-3746-4e93-93ed-f69690396dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = df_.sort_index(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363ad68f-5da9-49d0-b1a2-b18aeea2db02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note the order, otherwise it's not working.\n",
    "df = df_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e3f380-9c73-417f-96e0-ffb19c3af7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_['created_at'].head(-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957692bf-ba29-4338-aacf-692deb51ca54",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concat['created_at'] = pd.to_datetime(df_concat['created_at'], utc=True)\n",
    "df_concat = df_concat.sort_values(by=['created_at'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244aa8d3-3877-4260-997d-cd4724066b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ec54db-2499-4706-ad2b-def4dcda69ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(by=['created_at'])\n",
    "df['created_at'].head(-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb1b3bf-9ec8-4bc0-8577-1f772787b549",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"../Verkehrswende-Vergleich/Data/Verkehrswende/Verkehrswende_combined.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7491d897",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Hashtag harmonization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d9243990",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Verkehrswende\n",
    "df['clean_hashtags'] = df['clean_hashtags'].apply(\n",
    "    lambda x: x.replace(\"Verkehrs-Wende\", \"Verkehrswende\"))\n",
    "\n",
    "# Mobilitätswende\n",
    "df['clean_hashtags'] = df['clean_hashtags'].apply(\n",
    "    lambda x: x.replace(\"mobilitaetswende\", \"Mobilitätswende\"))\n",
    "df['clean_hashtags'] = df['clean_hashtags'].apply(\n",
    "    lambda x: x.replace(\"Mobilitaetswende\", \"Mobilitätswende\"))\n",
    "df['clean_hashtags'] = df['clean_hashtags'].apply(\n",
    "    lambda x: x.replace(\"Mobilitäts-Wende\", \"Mobilitätswende\"))\n",
    "\n",
    "# Energiewende\n",
    "df['clean_hashtags'] = df['clean_hashtags'].apply(\n",
    "    lambda x: x.replace(\"Energie-Wende\", \"Energiewende\"))\n",
    "\n",
    "# ÖPNV\n",
    "df['clean_hashtags'] = df['clean_hashtags'].apply(\n",
    "    lambda x: x.replace(\"öpnv\", \"ÖPNV\"))\n",
    "df['clean_hashtags'] = df['clean_hashtags'].apply(\n",
    "    lambda x: x.replace(\"oepnv\", \"ÖPNV\"))\n",
    "\n",
    "# E-Mobilität\n",
    "df['clean_hashtags'] = df['clean_hashtags'].apply(\n",
    "    lambda x: x.replace(\"emobilität\", \"E-Mobilität\"))\n",
    "df['clean_hashtags'] = df['clean_hashtags'].apply(\n",
    "    lambda x: x.replace(\"e-mobilität\", \"E-Mobilität\"))\n",
    "df['clean_hashtags'] = df['clean_hashtags'].apply(\n",
    "    lambda x: x.replace(\"emobility\", \"E-Mobilität\"))\n",
    "df['clean_hashtags'] = df['clean_hashtags'].apply(\n",
    "    lambda x: x.replace(\"Elektromobilität\", \"E-Mobilität\"))\n",
    "df['clean_hashtags'] = df['clean_hashtags'].apply(\n",
    "    lambda x: x.replace(\"Elektromobilitaet\", \"E-Mobilität\"))\n",
    "df['clean_hashtags'] = df['clean_hashtags'].apply(\n",
    "    lambda x: x.replace(\"ElektroMobilität\", \"E-Mobilität\"))\n",
    "\n",
    "# Radwege\n",
    "df['clean_hashtags'] = df['clean_hashtags'].apply(\n",
    "    lambda x: x.replace(\"radweg\", \"Radwege\"))\n",
    "\n",
    "# Grenzwerte\n",
    "df['clean_hashtags'] = df['clean_hashtags'].apply(\n",
    "    lambda x: x.replace(\"grenzwert\", \"Grenzwerte\"))\n",
    "df['clean_hashtags'] = df['clean_hashtags'].apply(\n",
    "    lambda x: x.replace(\"grenzwerte\", \"Grenzwerte\"))\n",
    "\n",
    "# Kleineres\n",
    "df['clean_hashtags'] = df['clean_hashtags'].apply(\n",
    "    lambda x: x.replace(\"klima\", \"Klima\"))\n",
    "df['clean_hashtags'] = df['clean_hashtags'].apply(\n",
    "    lambda x: x.replace(\"fahrrad\", \"Fahrrad\"))\n",
    "df['clean_hashtags'] = df['clean_hashtags'].apply(\n",
    "    lambda x: x.replace(\"autokorrektur\", \"Autokorrektur\"))\n",
    "df['clean_hashtags'] = df['clean_hashtags'].apply(\n",
    "    lambda x: x.replace(\"klimaschutz\", \"Klimaschutz\"))\n",
    "df['clean_hashtags'] = df['clean_hashtags'].apply(\n",
    "    lambda x: x.replace(\"klimapolitik\", \"Klimapolitik\"))\n",
    "df['clean_hashtags'] = df['clean_hashtags'].apply(\n",
    "    lambda x: x.replace(\"dannibleibt\", \"dannibleibt\"))\n",
    "df['clean_hashtags'] = df['clean_hashtags'].apply(\n",
    "    lambda x: x.replace(\"mobilität\", \"Mobilität\"))\n",
    "df['clean_hashtags'] = df['clean_hashtags'].apply(\n",
    "    lambda x: x.replace(\"autos\", \"Auto\"))\n",
    "df['clean_hashtags'] = df['clean_hashtags'].apply(\n",
    "    lambda x: x.replace(\"Autos\", \"Auto\"))\n",
    "df['clean_hashtags'] = df['clean_hashtags'].apply(\n",
    "    lambda x: x.replace(\"co2\", \"CO2\"))\n",
    "\n",
    "# All Lower Case\n",
    "\n",
    "df['clean_hashtags'] = df['clean_hashtags'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d20413db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                               none\n",
       "1                               none\n",
       "2                           eurobike\n",
       "3    auto fahrradfahrer sterben bahn\n",
       "4                    stuttgart21 s21\n",
       "Name: clean_hashtags, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['clean_hashtags'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6968c799",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"../Verkehrswende-Vergleich/Data/Verkehrswende/Verkehrswende_combined_hashtagsync.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
